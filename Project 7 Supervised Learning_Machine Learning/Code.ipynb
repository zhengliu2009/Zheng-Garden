{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFSCI 2725 Assignment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignmen group 2\n",
    "\n",
    "Luda Wang luw20@pitt.edu\n",
    "Kewei Li kel137@pitt.edu\n",
    "Zheng Liu zhengliu@pitt.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function can do the parameter tunning and feature selection using different kind of model.\n",
    "\n",
    "def train(features,label,model_name,para_dict={},feature_importance=False,feature_number=all,print_feature_name=False,feature_model=None,state=0):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import xgboost\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.feature_selection import chi2\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn import svm\n",
    "    \n",
    "    X=features.values\n",
    "    \n",
    "    if(type(label)==pd.core.frame.DataFrame):\n",
    "        label=label.values.ravel()\n",
    "        \n",
    "    if(feature_number!=all):\n",
    "        \n",
    "        from sklearn.feature_selection import SelectKBest\n",
    "    \n",
    "        fsm=SelectKBest(feature_model,k=feature_number)\n",
    "        X=fsm.fit_transform(X,label)\n",
    "        if(print_feature_name==True):\n",
    "            print(\"features we used\",features.keys()[fsm.get_support(indices=True)])\n",
    "    \n",
    "    if model_name=='xgb':\n",
    "        Model=xgb.XGBClassifier()\n",
    "        title='XGBoosting'\n",
    "    elif model_name=='ada':\n",
    "        Model=AdaBoostClassifier(random_state=state)\n",
    "        title='Adaptive Boost Classifier'\n",
    "    elif model_name=='bag':\n",
    "        Model=BaggingClassifier(random_state=state)\n",
    "        title='Bagging Classifier'\n",
    "    elif model_name=='rf':\n",
    "        Model=RandomForestClassifier(random_state=state)\n",
    "        title='Random Forest Classifier'\n",
    "    elif model_name=='lgr':\n",
    "        Model=LogisticRegression(random_state=state)\n",
    "        title='Logistic Regression'\n",
    "    elif model_name=='svc':\n",
    "        Model=svm.SVC(random_state=state)\n",
    "        title='Support Vector Classifier'\n",
    "    elif model_name=='dt':\n",
    "        Model=DecisionTreeClassifier(random_state=state)\n",
    "        title='Decision Tree Classifier'\n",
    "    grid=GridSearchCV(estimator=Model,param_grid=para_dict)\n",
    "    grid.fit(X,label)\n",
    "    print(\"Model name:\",title,\"\\n\"\"The best parameters are\",grid.best_params_)\n",
    "    print(\"Best Score is\",grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris_data Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of the iris_dataset is: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "iris_dataset=load_iris() \n",
    "print('Keys of the iris_dataset is:', iris_dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=pd.DataFrame(iris_dataset.data,columns=iris_dataset.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SVM classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features 1\n",
      "features we used Index(['petal length (cm)'], dtype='object')\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 101, 'degree': 4, 'kernel': 'poly'}\n",
      "Best Score is 0.96\n",
      "number of features 2\n",
      "features we used Index(['petal length (cm)', 'petal width (cm)'], dtype='object')\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 630, 'gamma': 0.0007000000000000001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9733333333333334\n",
      "number of features 3\n",
      "features we used Index(['sepal length (cm)', 'petal length (cm)', 'petal width (cm)'], dtype='object')\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 630, 'gamma': 0.0005, 'kernel': 'rbf'}\n",
      "Best Score is 0.98\n",
      "number of features 4\n",
      "features we used Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n",
      "       'petal width (cm)'],\n",
      "      dtype='object')\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 636, 'gamma': 0.0011000000000000003, 'kernel': 'rbf'}\n",
      "Best Score is 0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(1,5)):\n",
    "    print(\"number of features\",i)\n",
    "    train(feature,iris_dataset.target,'svc',[{'kernel': ['rbf'], \n",
    "                                          'gamma': np.arange(0.0005,0.0015,0.0001),\n",
    "                                          'C': list(range(630,660,1))},\n",
    "                                         {'kernel': ['linear'], \n",
    "                                          'C': list(range(1,1000,100))},\n",
    "                                         {'kernel':['poly'],\n",
    "                                          'C':list(range(1,1000,100)),\n",
    "                                          'degree':[2,3,4]}],\n",
    "          feature_model=chi2,\n",
    "          feature_number=i,\n",
    "          print_feature_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: \n",
    "\n",
    "we use Chi2 feature selection model and GridSearch method to do the feature selection, feature expansion(kernal) and parameter tunning. Within GridSearch, 3-fold cross validation is used automatically. The best score we get is using kernel 'rbf' and all 4 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features 1\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 3, 'penalty': 'l1'}\n",
      "Best Score is 0.9533333333333334\n",
      "number of features 2\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 5, 'penalty': 'l1'}\n",
      "Best Score is 0.9666666666666667\n",
      "number of features 3\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 5, 'penalty': 'l1'}\n",
      "Best Score is 0.9666666666666667\n",
      "number of features 4\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 61, 'penalty': 'l2'}\n",
      "Best Score is 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(1,5)):\n",
    "    print(\"number of features\",i)\n",
    "    train(feature,iris_dataset.target,'lgr',[{'penalty':['l1'],\n",
    "                                          'C':list(range(1,100))},\n",
    "                                         {'penalty':['l2'],\n",
    "                                          'C':list(range(1,100))}],\n",
    "          feature_number=i,feature_model=chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "All 4 features should be used to get the largest score when using Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features 1\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 51}\n",
      "Best Score is 0.9533333333333334\n",
      "number of features 2\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 31}\n",
      "Best Score is 0.9733333333333334\n",
      "number of features 3\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 31}\n",
      "Best Score is 0.9733333333333334\n",
      "number of features 4\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 31}\n",
      "Best Score is 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(1,5)):\n",
    "    print(\"number of features\",i)\n",
    "    train(feature,iris_dataset.target,'rf',{'max_features':['auto','log2',None],\n",
    "                                        'n_estimators':list(range(1,100,10)),\n",
    "                                        'min_samples_leaf':list(range(1,10,1))},\n",
    "      feature_model=chi2,\n",
    "      feature_number=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "As we can see, the score is not changed when adding more features after 2. Therefore, to avoid overfitting, we can use 2 features when apply Random Forest Classifier. Two features we selected from Chi2 score are 'petal length (cm)' and 'petal width (cm)'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features 1\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 1.0999999999999999, 'n_estimators': 13}\n",
      "Best Score is 0.9533333333333334\n",
      "number of features 2\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 1.1999999999999997, 'n_estimators': 8}\n",
      "Best Score is 0.9733333333333334\n",
      "number of features 3\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 1.1999999999999997, 'n_estimators': 5}\n",
      "Best Score is 0.9733333333333334\n",
      "number of features 4\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 1.1999999999999997, 'n_estimators': 8}\n",
      "Best Score is 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(1,5)):\n",
    "    print(\"number of features\",i)\n",
    "    train(feature,iris_dataset.target,'ada',{'n_estimators':list(range(5,15,1)),\n",
    "                                         'learning_rate':np.arange(0.5,1.5,0.1)},\n",
    "          feature_model=chi2,\n",
    "      feature_number=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "Two features we need to use. same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features 1\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Score is 0.94\n",
      "number of features 2\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Score is 0.96\n",
      "number of features 3\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Score is 0.96\n",
      "number of features 4\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Score is 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(1,5)):\n",
    "    print(\"number of features\",i)\n",
    "    train(feature,iris_dataset.target,'dt',{'max_depth':list(range(1,15,1)),\n",
    "                                       'min_samples_leaf':list(range(1,10,1)),\n",
    "                                       'min_samples_split':list(range(2,10,1))},\n",
    "          feature_model=chi2,\n",
    "          feature_number=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "4 features should be used to use Decision Tree Classifier.\n",
    "\n",
    "The highest score we get is from SVM classifier with all 4 features and kernal 'rbf' . The best score is 0.993."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congressional Voting Records Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes=pd.read_table('house-votes-84.txt',index_col=None)\n",
    "\n",
    "target_vote=pd.concat([votes[['Party']]])\n",
    "features_vote=votes.drop(['Party'], axis=1)\n",
    "\n",
    "features_vote.replace(to_replace='y',value=1,inplace=True)\n",
    "features_vote.replace(to_replace='w',value=2,inplace=True)\n",
    "features_vote.replace(to_replace='n',value=0,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features 1\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 11, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 2\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 21, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 3\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 11, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9471264367816092\n",
      "number of features 4\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 31, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9471264367816092\n",
      "number of features 5\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 51, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 6\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 41, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 7\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 41, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 8\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 41, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 9\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 61, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 10\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 61, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9425287356321839\n",
      "number of features 11\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 41, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9425287356321839\n",
      "number of features 12\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 31, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9402298850574713\n",
      "number of features 13\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 51, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9425287356321839\n",
      "number of features 14\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 91, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9425287356321839\n",
      "number of features 15\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 71, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9471264367816092\n",
      "number of features 16\n",
      "Model name: Support Vector Classifier \n",
      "The best parameters are {'C': 61, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Score is 0.9494252873563218\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(1,17)):\n",
    "    print(\"number of features\",i)\n",
    "    train(features_vote,target_vote,'svc',[{'kernel': ['rbf'], \n",
    "                                        'gamma': [1e-3, 1e-4],\n",
    "                                        'C': list(range(1,100,10))},\n",
    "                                       {'kernel': ['linear'], \n",
    "                                        'C': list(range(1,1000,100))}],feature_number=i,feature_model=chi2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comment:\n",
    "    \n",
    "The best number of features is 3,kernel is 'rbf',and the features we choose are 'adoption_of_the_budget_resolution', 'physician_fee_freeze' and 'education_spending'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features 1\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 1, 'penalty': 'l1'}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 2\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 1, 'penalty': 'l1'}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 3\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 1, 'penalty': 'l1'}\n",
      "Best Score is 0.9333333333333333\n",
      "number of features 4\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 1, 'penalty': 'l1'}\n",
      "Best Score is 0.9287356321839081\n",
      "number of features 5\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 1, 'penalty': 'l1'}\n",
      "Best Score is 0.9310344827586207\n",
      "number of features 6\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 9, 'penalty': 'l2'}\n",
      "Best Score is 0.9333333333333333\n",
      "number of features 7\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 1, 'penalty': 'l1'}\n",
      "Best Score is 0.9310344827586207\n",
      "number of features 8\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 7, 'penalty': 'l1'}\n",
      "Best Score is 0.9310344827586207\n",
      "number of features 9\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 2, 'penalty': 'l1'}\n",
      "Best Score is 0.9287356321839081\n",
      "number of features 10\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 15, 'penalty': 'l2'}\n",
      "Best Score is 0.9287356321839081\n",
      "number of features 11\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 2, 'penalty': 'l1'}\n",
      "Best Score is 0.9333333333333333\n",
      "number of features 12\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 2, 'penalty': 'l1'}\n",
      "Best Score is 0.9471264367816092\n",
      "number of features 13\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 9, 'penalty': 'l2'}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 14\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 6, 'penalty': 'l2'}\n",
      "Best Score is 0.9425287356321839\n",
      "number of features 15\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 1, 'penalty': 'l1'}\n",
      "Best Score is 0.9494252873563218\n",
      "number of features 16\n",
      "Model name: Logistic Regression \n",
      "The best parameters are {'C': 1, 'penalty': 'l1'}\n",
      "Best Score is 0.9471264367816092\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(1,17)):\n",
    "    print(\"number of features\",i)\n",
    "    train(features_vote,target_vote,'lgr',[{'penalty':['l1'],\n",
    "                                        'C':list(range(1,100))},\n",
    "                                       {'penalty':['l2'],\n",
    "                                        'C':list(range(1,100))}],feature_model=chi2,feature_number=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "Logistic Regression uses 15 features to get the best score. The best score is 0.9494"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features 1\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 15}\n",
      "Best Score is 0.9563218390804598\n",
      "number of features 2\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 15}\n",
      "Best Score is 0.9586206896551724\n",
      "number of features 3\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 16}\n",
      "Best Score is 0.9563218390804598\n",
      "number of features 4\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': 'auto', 'min_samples_leaf': 5, 'n_estimators': 20}\n",
      "Best Score is 0.9563218390804598\n",
      "number of features 5\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': None, 'min_samples_leaf': 4, 'n_estimators': 17}\n",
      "Best Score is 0.9586206896551724\n",
      "number of features 6\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': None, 'min_samples_leaf': 4, 'n_estimators': 15}\n",
      "Best Score is 0.9586206896551724\n",
      "number of features 7\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': None, 'min_samples_leaf': 4, 'n_estimators': 19}\n",
      "Best Score is 0.9586206896551724\n",
      "number of features 8\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': None, 'min_samples_leaf': 5, 'n_estimators': 23}\n",
      "Best Score is 0.9563218390804598\n",
      "number of features 9\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 17}\n",
      "Best Score is 0.9586206896551724\n",
      "number of features 10\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': 'auto', 'min_samples_leaf': 3, 'n_estimators': 23}\n",
      "Best Score is 0.9586206896551724\n",
      "number of features 11\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': None, 'min_samples_leaf': 4, 'n_estimators': 24}\n",
      "Best Score is 0.9586206896551724\n",
      "number of features 12\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 24}\n",
      "Best Score is 0.9724137931034482\n",
      "number of features 13\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': None, 'min_samples_leaf': 1, 'n_estimators': 17}\n",
      "Best Score is 0.9724137931034482\n",
      "number of features 14\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': None, 'min_samples_leaf': 4, 'n_estimators': 15}\n",
      "Best Score is 0.9724137931034482\n",
      "number of features 15\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': None, 'min_samples_leaf': 1, 'n_estimators': 22}\n",
      "Best Score is 0.9747126436781609\n",
      "number of features 16\n",
      "Model name: Random Forest Classifier \n",
      "The best parameters are {'max_features': None, 'min_samples_leaf': 1, 'n_estimators': 22}\n",
      "Best Score is 0.9747126436781609\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(1,17)):\n",
    "    print(\"number of features\",i)\n",
    "    train(features_vote,target_vote,'rf',{'max_features':['auto','log2',None],\n",
    "                                      'n_estimators':list(range(15,25,1)),\n",
    "                                      'min_samples_leaf':list(range(1,10,1))},feature_number=i,feature_model=chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "15 features are selected applying Random Forest Classier. The best score is 0.9747."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features 1\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.011, 'n_estimators': 51}\n",
      "Best Score is 0.9471264367816092\n",
      "number of features 2\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.011, 'n_estimators': 71}\n",
      "Best Score is 0.9471264367816092\n",
      "number of features 3\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.001, 'n_estimators': 1}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 4\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.001, 'n_estimators': 1}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 5\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.001, 'n_estimators': 1}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 6\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.001, 'n_estimators': 1}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 7\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.001, 'n_estimators': 1}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 8\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.001, 'n_estimators': 1}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 9\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.001, 'n_estimators': 1}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 10\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.001, 'n_estimators': 1}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 11\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.001, 'n_estimators': 1}\n",
      "Best Score is 0.9448275862068966\n",
      "number of features 12\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.011, 'n_estimators': 81}\n",
      "Best Score is 0.9471264367816092\n",
      "number of features 13\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.011, 'n_estimators': 81}\n",
      "Best Score is 0.9471264367816092\n",
      "number of features 14\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.011, 'n_estimators': 81}\n",
      "Best Score is 0.9471264367816092\n",
      "number of features 15\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.011, 'n_estimators': 81}\n",
      "Best Score is 0.9471264367816092\n",
      "number of features 16\n",
      "Model name: Adaptive Boost Classifier \n",
      "The best parameters are {'learning_rate': 0.011, 'n_estimators': 81}\n",
      "Best Score is 0.9471264367816092\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(1,17)):\n",
    "    print(\"number of features\",i)\n",
    "    train(features_vote,target_vote,'ada',{'n_estimators':list(range(1,100,10)),\n",
    "                                       'learning_rate':np.arange(0.001,0.012,0.01)},\n",
    "      feature_model=chi2,\n",
    "      feature_number=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "    \n",
    "3 features are selected applying adaptive boosting model. The best score is 0.9448"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features 1\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Score is 0.9563218390804598\n",
      "number of features 2\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Score is 0.9563218390804598\n",
      "number of features 3\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best Score is 0.9563218390804598\n",
      "number of features 4\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Score is 0.9563218390804598\n",
      "number of features 5\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Score is 0.9563218390804598\n",
      "number of features 6\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Score is 0.9563218390804598\n",
      "number of features 7\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Score is 0.9563218390804598\n",
      "number of features 8\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 4, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Best Score is 0.9517241379310345\n",
      "number of features 9\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 4, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Best Score is 0.9517241379310345\n",
      "number of features 10\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "Best Score is 0.960919540229885\n",
      "number of features 11\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 9}\n",
      "Best Score is 0.960919540229885\n",
      "number of features 12\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "Best Score is 0.9724137931034482\n",
      "number of features 13\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "Best Score is 0.9701149425287356\n",
      "number of features 14\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 6, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "Best Score is 0.9724137931034482\n",
      "number of features 15\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "Best Score is 0.9701149425287356\n",
      "number of features 16\n",
      "Model name: Decision Tree Classifier \n",
      "The best parameters are {'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "Best Score is 0.9701149425287356\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(1,17)):\n",
    "    print(\"number of features\",i)\n",
    "    train(features_vote,target_vote,'dt',{'max_depth':list(range(1,15,1)),\n",
    "                                       'min_samples_leaf':list(range(1,10,1)),\n",
    "                                       'min_samples_split':list(range(2,10,1))},feature_model=chi2,feature_number=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "12 features are selected applying Decision Tree Classier. The best Score is 0.9724\n",
    "\n",
    "The best score we get is from model Decision Tree Classier. The best score is 0.9724"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
